version: '3.8'

services:
  namenode:
    image: bde2020/hadoop-namenode:latest
    container_name: namenode
    environment:
      - CLUSTER_NAME=hadoop_cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=1
    ports:
      - "9870:9870"  # Web UI
      - "9000:9000"  # HDFS
    volumes:
      - namenode_data:/hadoop/dfs/name
    networks:
      - hadoop_network

  datanode:
    image: bde2020/hadoop-datanode:latest
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_replication=1
    ports:
      - "9864:9864"  # Web UI
    volumes:
      - datanode_data:/hadoop/dfs/data
    depends_on:
      - namenode
    networks:
      - hadoop_network

  python:
    image: python:3.10
    container_name: python3
    working_dir: /usr/local/spark/app
    volumes:
      - ../spark/app:/usr/local/spark/app
      - ../spark/resources:/usr/local/spark/resources
      - ../datasets/source_crm:/usr/local/datasets/source_crm
      - ../requirements.txt:/usr/local/requirements.txt
    networks:
      - hadoop_network
    command: >
        bash -c "
          pip install -r /usr/local/requirements.txt &&
          tail -f /dev/null
        "

  postgres:
    image: postgres:latest
    container_name: postgres_dw
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
    restart: always
    networks:
      - hadoop_network

  mysql:
    image: mysql:latest
    container_name: mysql_source
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: source_erp
      MYSQL_USER: etl
      MYSQL_PASSWORD: etl
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./mysql-init:/docker-entrypoint-initdb.d  # Chứa script SQL hoặc CSV import
      - ../datasets/source_erp:/var/lib/mysql-files  # Chứa file CSV để import vào MySQL
    networks:
      - hadoop_network


  airflow-webserver:
    build:
      context: ..
      dockerfile: Dockerfile
    container_name: airflow_webserver
    command: webserver
    environment:
      - LOAD_EX=y
      - EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__SECRET_KEY=airflow_secret_key
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    ports:
      - "8081:8080"
    volumes:
      - ../airflow-dags:/opt/airflow/dags
      - ../datasets/source_crm:/usr/local/datasets/source_crm
      - ../spark/app:/usr/local/spark/app
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    networks:
      - hadoop_network

  airflow-scheduler:
    build:
      context: ..
      dockerfile: Dockerfile
    container_name: airflow_scheduler
    command: scheduler
    environment:
      - EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__SECRET_KEY=airflow_secret_key
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ../airflow-dags:/opt/airflow/dags
      - ../datasets/source_crm:/usr/local/datasets/source_crm
      - ../spark/app:/usr/local/spark/app
    depends_on:
      airflow-webserver:
        condition: service_started
      airflow-init:
        condition: service_completed_successfully
    networks:
        - hadoop_network

  airflow-init:
    build:
      context: ..
      dockerfile: Dockerfile
    container_name: airflow_init
    # Run db init and user creation via default entrypoint
    entrypoint: /bin/bash
    command: -c "
      sleep 10 &&
      airflow db init &&
      airflow users create \
        --username admin \
        --password admin \
        --firstname Admin \
        --lastname User \
        --role Admin \
        --email admin@example.com
      "
    environment:
      - EXECUTOR=LocalExecutor
      - AIRFLOW__WEBSERVER__SECRET_KEY=airflow_secret_key
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    depends_on:
      - postgres
    networks:
      - hadoop_network

  # Service to set up connections
  airflow-setup:
    build:
      context: ..
      dockerfile: Dockerfile
    container_name: airflow_setup
    entrypoint: /bin/bash
    command: -c "
      sleep 60 &&
      echo 'Creating Spark connection...' &&
      airflow connections delete spark_default || true &&
      airflow connections add 'spark_default' \
        --conn-type 'spark' \
        --conn-host 'spark://spark' \
        --conn-port '7077' \
        --conn-extra '{\"deploy-mode\":\"client\"}' &&
      echo 'Spark connection created successfully!'
      "
    environment:
      - EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__SECRET_KEY=airflow_secret_key
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    depends_on:
      - airflow-webserver
      - airflow-init
    networks:
      - hadoop_network
    restart: on-failure


    # Spark with 3 workers
  spark:
    image: bitnami/spark:3.5  # Updated Spark version
    user: root
    hostname: spark
    container_name: spark
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ../spark/app:/usr/local/spark/app
      - ../spark/resources:/usr/local/spark/resources
    ports:
      - "8181:8080"
      - "7077:7077"
    networks:
      - hadoop_network

  spark-worker-1:
    image: bitnami/spark:3.5  # Updated Spark version
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077 # MUST config in Airflow Connection
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ../spark/app:/usr/local/spark/app
      - ../spark/resources:/usr/local/spark/resources
    networks:
      - hadoop_network


  spark-worker-2:
    image: bitnami/spark:3.5  # Updated Spark version
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ../spark/app:/usr/local/spark/app
      - ../spark/resources:/usr/local/spark/resources
    networks:
      - hadoop_network

  jupyter-spark:
    image: jupyter/pyspark-notebook:spark-3.5.0 # Updated to match Spark version (find appropriate tag if needed)
    container_name: jupyter
    ports:
      - "8888:8888"
      - "4040-4080:4040-4080"
    volumes:
      - ../notebooks:/home/work
    networks:
      - hadoop_network


volumes:
  namenode_data:
  datanode_data:
  postgres_data:
  mysql_data:

networks:
  hadoop_network:
    driver: bridge
